{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05b25a6e-5806-44d5-bcd5-75736b38b68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER Sentiment Analysis Results\n",
      "\n",
      "Sentence: I am so happy with the service.\n",
      "Scores: {'neg': 0.0, 'neu': 0.559, 'pos': 0.441, 'compound': 0.6948}\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Sentence: This movie was a waste of time.\n",
      "Scores: {'neg': 0.318, 'neu': 0.682, 'pos': 0.0, 'compound': -0.4215}\n",
      "Sentiment: Negative\n",
      "--------------------------------------------------\n",
      "Sentence: It was an okay experience.\n",
      "Scores: {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.2263}\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Sentence: Best purchase I've made in years!\n",
      "Scores: {'neg': 0.0, 'neu': 0.527, 'pos': 0.473, 'compound': 0.6696}\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Sentence: I don't like this app, it's too slow.\n",
      "Scores: {'neg': 0.232, 'neu': 0.768, 'pos': 0.0, 'compound': -0.2755}\n",
      "Sentiment: Negative\n",
      "--------------------------------------------------\n",
      "\n",
      "Answers to the Questions\n",
      "\n",
      "1. What do the sentiment scores represent?\n",
      "-pos: Percentage of positive words in the sentence\n",
      "-neu:Percentage of neutral words\n",
      "-neg:Percentage of negative words\n",
      "-compound: A single score between -1 and +1 summarizing total sentiment\n",
      "    +1 = extremely positive\n",
      "    -1 = extremely negative\n",
      "The compound score is the main value used for classification.\n",
      "\n",
      "\n",
      "2. How do you classify a sentence using the compound score?\n",
      "Use VADER's rule:\n",
      "compound ≥ 0.05: Positive\n",
      "compound ≤ -0.05: Negative\n",
      "between -0.05 and 0.05: Neutral\n",
      "These thresholds are standard for VADER and commonly used in sentiment-analysis projects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Vader Sentiment Analysis\n",
    "# Step 1: Install and import VADER\n",
    "#!pip install vaderSentiment nltk\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Step 2: Define sample texts\n",
    "sentences1 = [\n",
    "    \"I am so happy with the service.\",\n",
    "    \"This movie was a waste of time.\",\n",
    "    \"It was an okay experience.\",\n",
    "    \"Best purchase I've made in years!\",\n",
    "    \"I don't like this app, it's too slow.\"\n",
    "]\n",
    "\n",
    "# Step 3: Create the analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"VADER Sentiment Analysis Results\")\n",
    "print ()\n",
    "\n",
    "# Step 4: Analyze each sentence\n",
    "for text in sentences1:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    print(f\"Sentence: {text}\")\n",
    "    print(f\"Scores: {scores}\")\n",
    "\n",
    "    # Classification using compound score\n",
    "    compound = scores['compound']\n",
    "    if compound >= 0.05:\n",
    "        sentiment = \"Positive\"\n",
    "    elif compound <= -0.05:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "#Question 1 and Question 2\n",
    "\n",
    "print(\"\\nAnswers to the Questions\\n\")\n",
    "\n",
    "print(\"1. What do the sentiment scores represent?\")\n",
    "print(\"-pos: Percentage of positive words in the sentence\")\n",
    "print(\"-neu:Percentage of neutral words\")\n",
    "print(\"-neg:Percentage of negative words\")\n",
    "print(\"-compound: A single score between -1 and +1 summarizing total sentiment\")\n",
    "print(\"    +1 = extremely positive\")\n",
    "print(\"    -1 = extremely negative\")\n",
    "print(\"The compound score is the main value used for classification.\\n\")\n",
    "print(\"\\n2. How do you classify a sentence using the compound score?\")\n",
    "print(\"Use VADER's rule:\")\n",
    "print(\"compound ≥ 0.05: Positive\")\n",
    "print(\"compound ≤ -0.05: Negative\")\n",
    "print(\"between -0.05 and 0.05: Neutral\")\n",
    "print(\"These thresholds are standard for VADER and commonly used in sentiment-analysis projects.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "789409f5-5123-44a8-801d-8290588d154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Huggingface Sentiment Analysis\n",
    "#Step 1 Install transformers\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63adaa5f-eac5-4057-aadb-602ece8b3ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Huggingface Transformers Sentiment Analysis Results\n",
      "\n",
      "Sentence: I love this new phone.\n",
      "Label: POSITIVE | Confidence: 0.9998\n",
      "--------------------------------------------------\n",
      "Sentence: I had a terrible experience with customer support.\n",
      "Label: NEGATIVE | Confidence: 0.9995\n",
      "--------------------------------------------------\n",
      "Sentence: The movie was not bad, but not great either.\n",
      "Label: NEGATIVE | Confidence: 0.9963\n",
      "--------------------------------------------------\n",
      "Sentence: Absolutely loved the restaurant!\n",
      "Label: POSITIVE | Confidence: 0.9999\n",
      "--------------------------------------------------\n",
      "Sentence: The product arrived damaged, very disappointed.\n",
      "Label: NEGATIVE | Confidence: 0.9998\n",
      "--------------------------------------------------\n",
      "Answers to the Questions\n",
      "\n",
      "1. What labels does the Huggingface model return?\n",
      "The SST-2 model outputs:\n",
      "-POSITIVE\n",
      "-NEGATIVE\n",
      "\n",
      "2. What do confidence scores mean?\n",
      "Confidence score:probability of the model being correct\n",
      "  Ranges from 0 to 1\n",
      "  Higher number → higher certainty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#Step 2 Load DistilBERT sentiment model\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "print(\"\\nHuggingface Transformers Sentiment Analysis Results\")\n",
    "print ()\n",
    "\n",
    "#Step 3 Sample texts\n",
    "sentences2 = [\n",
    "    \"I love this new phone.\",\n",
    "    \"I had a terrible experience with customer support.\",\n",
    "    \"The movie was not bad, but not great either.\",\n",
    "    \"Absolutely loved the restaurant!\",\n",
    "    \"The product arrived damaged, very disappointed.\"\n",
    "]\n",
    "\n",
    "#Step 4 Run analysis\n",
    "for text in sentences2:\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    print(f\"Sentence: {text}\")\n",
    "    print(f\"Label: {result['label']} | Confidence: {result['score']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Answers to the Questions\\n\")\n",
    "\n",
    "print(\"1. What labels does the Huggingface model return?\")\n",
    "print(\"The SST-2 model outputs:\")\n",
    "print(\"-POSITIVE\")\n",
    "print(\"-NEGATIVE\\n\")\n",
    "\n",
    "print(\"2. What do confidence scores mean?\")\n",
    "print(\"Confidence score:probability of the model being correct\")\n",
    "print(\"  Ranges from 0 to 1\")\n",
    "print(\"  Higher number → higher certainty\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "148138ab-c115-498f-98c9-1f2bc3741014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARISON: VADER vs HUGGINGFACE\n",
      "\n",
      "Sentence: I love this new phone.\n",
      "  VADER → Label: Positive, Compound: 0.6369\n",
      "  HF    → Label: POSITIVE, Confidence: 0.9998\n",
      "------------------------------------------------------------\n",
      "Sentence: I had a terrible experience with customer support.\n",
      "  VADER → Label: Negative, Compound: -0.1027\n",
      "  HF    → Label: NEGATIVE, Confidence: 0.9995\n",
      "------------------------------------------------------------\n",
      "Sentence: The movie was not bad, but not great either.\n",
      "  VADER → Label: Negative, Compound: -0.5448\n",
      "  HF    → Label: NEGATIVE, Confidence: 0.9963\n",
      "------------------------------------------------------------\n",
      "Sentence: Absolutely loved the restaurant!\n",
      "  VADER → Label: Positive, Compound: 0.6689\n",
      "  HF    → Label: POSITIVE, Confidence: 0.9999\n",
      "------------------------------------------------------------\n",
      "Sentence: The product arrived damaged, very disappointed.\n",
      "  VADER → Label: Negative, Compound: -0.7425\n",
      "  HF    → Label: NEGATIVE, Confidence: 0.9998\n",
      "------------------------------------------------------------\n",
      "\n",
      "Answers to the Questions\n",
      "\n",
      "1. “How do the results compare?”\n",
      "\n",
      "On this dataset, VADER and Huggingface give the same sentiment label for all 5 sentences (both say\n",
      "POSITIVE or NEGATIVE for each one). However, Huggingface gives very high confidence scores close\n",
      "to 1, while VADER’s compound scores are more moderate (for example, –0.1027 vs. 0.9995), which\n",
      "shows that Huggingface is more confident in its predictions.\n",
      "\n",
      "2. “Which is more accurate for complex sentences?”\n",
      "\n",
      "In the results, the more complex sentence “The movie was not bad, but not great either.” is classified\n",
      "as NEGATIVE by both models, but VADER’s compound score (–0.5448) shows stronger negativity than\n",
      "Huggingface’s confidence (0.9963). You can say that for this specific case both models agree, but in\n",
      "general Huggingface is expected to handle nuance better because it is a deep learning model.\n",
      "\n",
      "3. “Which is faster?”\n",
      "\n",
      "The code shows VADER runs almost instantly, while Huggingface takes longer to load and run the\n",
      "model. So you can still say: VADER is faster because it is rule-based, while Huggingface is slower but\n",
      "more powerful because it uses a large neural network.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Compare VADER and Huggingface\n",
    "#Create the analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"COMPARISON: VADER vs HUGGINGFACE\\n\")\n",
    "\n",
    "for text in sentences2:\n",
    "    #Vader\n",
    "    vader_scores = vader.polarity_scores(text)\n",
    "    vader_compound = vader_scores[\"compound\"]\n",
    "    if vader_compound >= 0.05:\n",
    "        vader_label = \"Positive\"\n",
    "    elif vader_compound <= -0.05:\n",
    "        vader_label = \"Negative\"\n",
    "    else:\n",
    "        vader_label = \"Neutral\"\n",
    "\n",
    "    # Huggingface\n",
    "    hf_result = sentiment_pipeline(text)[0]\n",
    "    hf_label = hf_result['label']\n",
    "    hf_conf = hf_result['score']\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Sentence: {text}\")\n",
    "    print(f\"  VADER → Label: {vader_label}, Compound: {vader_compound}\")\n",
    "    print(f\"  HF    → Label: {hf_label}, Confidence: {hf_conf:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nAnswers to the Questions\\n\")\n",
    "print(\"1. “How do the results compare?”\\n\")\n",
    "print(\"On this dataset, VADER and Huggingface give the same sentiment label for all 5 sentences (both say\")\n",
    "print(\"POSITIVE or NEGATIVE for each one). However, Huggingface gives very high confidence scores close\")\n",
    "print(\"to 1, while VADER’s compound scores are more moderate (for example, –0.1027 vs. 0.9995), which\")\n",
    "print(\"shows that Huggingface is more confident in its predictions.\\n\")\n",
    "\n",
    "print(\"2. “Which is more accurate for complex sentences?”\\n\")\n",
    "print(\"In the results, the more complex sentence “The movie was not bad, but not great either.” is classified\")\n",
    "print(\"as NEGATIVE by both models, but VADER’s compound score (–0.5448) shows stronger negativity than\")\n",
    "print(\"Huggingface’s confidence (0.9963). You can say that for this specific case both models agree, but in\")\n",
    "print(\"general Huggingface is expected to handle nuance better because it is a deep learning model.\\n\")\n",
    "\n",
    "print(\"3. “Which is faster?”\\n\")\n",
    "print(\"The code shows VADER runs almost instantly, while Huggingface takes longer to load and run the\")\n",
    "print(\"model. So you can still say: VADER is faster because it is rule-based, while Huggingface is slower but\")\n",
    "print(\"more powerful because it uses a large neural network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f765a01-21d3-45c6-a28b-a15bb283f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Evaluation of VADER vs Huggingface\n",
    "\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e4e20a2-44ab-41d7-8fbe-9a86fed35275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions\n",
      "  Sentence3                                   True Sentiment VADER Predicted  \\\n",
      "0           This book was absolutely amazing!  positive       positive         \n",
      "1               I really disliked this novel.  negative        neutral         \n",
      "2     The story was okay but nothing special.   neutral       negative         \n",
      "3     One of the best books I have ever read.  positive       positive         \n",
      "4  The plot was confusing and poorly written.  negative       negative         \n",
      "\n",
      "  Huggingface Predicted  \n",
      "0  positive              \n",
      "1  negative              \n",
      "2  negative              \n",
      "3  positive              \n",
      "4  negative              \n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Evaluation Metrics\n",
      "\n",
      "VADER Performance:\n",
      "Accuracy:  0.600\n",
      "Precision: 0.600\n",
      "Recall:    0.600\n",
      "F1-Score:  0.600\n",
      "\n",
      "------------------------------------------------------------\n",
      "Huggingface Performance:\n",
      "Accuracy:  0.800\n",
      "Precision: 0.667\n",
      "Recall:    0.800\n",
      "F1-Score:  0.720\n",
      "------------------------------------------------------------\n",
      "\n",
      "Answers to the Questions\n",
      "\n",
      "1. Model Performance (Accuracy, Precision, Recall, F1-Score)\n",
      "- Huggingface generally scores higher because it is a deep learning model trained on large datasets.\n",
      "- VADER performs reasonably well but struggles with neutral or mixed sentences, lowering its metrics.\n",
      "\n",
      "2. Which Model Performs Better?\n",
      "- Positive Sentiment: Huggingface performs better with stronger, more confident positive predictions.\n",
      "- Negative Sentiment: Huggingface also performs better by capturing context like 'confusing' or 'poorly written.'\n",
      "\n",
      "3. Causes of Prediction Discrepancies\n",
      "- VADER is rule-based and may misinterpret nuanced or mixed sentiment due to limited lexical rules.\n",
      "- Huggingface uses contextual deep learning, so it understands phrasing better and produces different predictions on subtle sentences.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create Test Dataset\n",
    "data = {\n",
    "    \"Sentence3\": [\n",
    "        \"This book was absolutely amazing!\",\n",
    "        \"I really disliked this novel.\",\n",
    "        \"The story was okay but nothing special.\",\n",
    "        \"One of the best books I have ever read.\",\n",
    "        \"The plot was confusing and poorly written.\"\n",
    "    ],\n",
    "    \n",
    "    # TRUE labels for evaluation\n",
    "    \"True Sentiment\": [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Perform Sentiment Analysis\n",
    "# Load VADER\n",
    "vaderanalyzer = SentimentIntensityAnalyzer()\n",
    "# Load Huggingface model\n",
    "huggingface = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Function to map VADER compound → label\n",
    "def vaderanalyzer_label(score):\n",
    "    if score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Apply both models\n",
    "vaderanalyzer_preds = []\n",
    "huggingface_preds = []\n",
    "\n",
    "for text in df[\"Sentence3\"]:\n",
    "    # VADER\n",
    "    compound = vaderanalyzer.polarity_scores(text)[\"compound\"]\n",
    "    vaderanalyzer_preds.append(vaderanalyzer_label(compound))\n",
    "\n",
    "    # Huggingface\n",
    "    huggingface_result = huggingface(text)[0][\"label\"]\n",
    "    huggingface_preds.append(huggingface_result.lower())   \n",
    "\n",
    "\n",
    "df[\"VADER Predicted\"] = vaderanalyzer_preds\n",
    "df[\"Huggingface Predicted\"] = huggingface_preds\n",
    "\n",
    "print(\"\\nPredictions\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Step 3: Calculate Evaluation Metrics\n",
    "true = df[\"True Sentiment\"]\n",
    "\n",
    "# VADER metrics\n",
    "vaderanalyzer_acc = accuracy_score(true, df[\"VADER Predicted\"])\n",
    "vaderanalyzer_prec = precision_score(true, df[\"VADER Predicted\"], average=\"weighted\", zero_division=0)\n",
    "vaderanalyzer_rec = recall_score(true, df[\"VADER Predicted\"], average=\"weighted\", zero_division=0)\n",
    "vaderanalyzer_f1 = f1_score(true, df[\"VADER Predicted\"], average=\"weighted\", zero_division=0)\n",
    "\n",
    "# Huggingface metrics\n",
    "huggingface_acc = accuracy_score(true, df[\"Huggingface Predicted\"])\n",
    "huggingface_prec = precision_score(true, df[\"Huggingface Predicted\"], average=\"weighted\", zero_division=0)\n",
    "huggingface_rec = recall_score(true, df[\"Huggingface Predicted\"], average=\"weighted\", zero_division=0)\n",
    "huggingface_f1 = f1_score(true, df[\"Huggingface Predicted\"], average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Evaluation Metrics\\n\")\n",
    "\n",
    "print(\"VADER Performance:\")\n",
    "print(f\"Accuracy:  {vaderanalyzer_acc:.3f}\")\n",
    "print(f\"Precision: {vaderanalyzer_prec:.3f}\")\n",
    "print(f\"Recall:    {vaderanalyzer_rec:.3f}\")\n",
    "print(f\"F1-Score:  {vaderanalyzer_f1:.3f}\\n\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"Huggingface Performance:\")\n",
    "print(f\"Accuracy:  {huggingface_acc:.3f}\")\n",
    "print(f\"Precision: {huggingface_prec:.3f}\")\n",
    "print(f\"Recall:    {huggingface_rec:.3f}\")\n",
    "print(f\"F1-Score:  {huggingface_f1:.3f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nAnswers to the Questions\\n\")\n",
    "print(\"1. Model Performance (Accuracy, Precision, Recall, F1-Score)\")\n",
    "print(\"- Huggingface generally scores higher because it is a deep learning model trained on large datasets.\")\n",
    "print(\"- VADER performs reasonably well but struggles with neutral or mixed sentences, lowering its metrics.\\n\")\n",
    "\n",
    "print(\"2. Which Model Performs Better?\")\n",
    "print(\"- Positive Sentiment: Huggingface performs better with stronger, more confident positive predictions.\")\n",
    "print(\"- Negative Sentiment: Huggingface also performs better by capturing context like 'confusing' or 'poorly written.'\\n\")\n",
    "\n",
    "print(\"3. Causes of Prediction Discrepancies\")\n",
    "print(\"- VADER is rule-based and may misinterpret nuanced or mixed sentiment due to limited lexical rules.\")\n",
    "print(\"- Huggingface uses contextual deep learning, so it understands phrasing better and produces different predictions on subtle sentences.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
