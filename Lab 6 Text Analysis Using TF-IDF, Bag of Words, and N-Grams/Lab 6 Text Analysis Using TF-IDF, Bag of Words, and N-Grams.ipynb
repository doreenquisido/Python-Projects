{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b04d5f4c-1eb5-44c7-b08a-40840b3c5d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Set up\n",
      "\n",
      "Documents:\n",
      "['Data science is an interdisciplinary field.', 'Machine learning is a subset of artificial intelligence.', 'Data science uses machine learning algorithms.', 'Artificial intelligence and data science are growing fields.']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set up\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "# Sample corpus\n",
    "documents = [\n",
    "\"Data science is an interdisciplinary field.\",\n",
    "\"Machine learning is a subset of artificial intelligence.\",\n",
    "\"Data science uses machine learning algorithms.\",\n",
    "\"Artificial intelligence and data science are growing fields.\"\n",
    "]\n",
    "\n",
    "print (\"\\nStep 1: Set up\")\n",
    "print(\"\\nDocuments:\")\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0de5ed0-38d9-434d-a47f-565f8a8f6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step2: Bag of Words\n",
      "\n",
      "Bag of Words Model:\n",
      "\n",
      "       algorithms  an  and  are  artificial  data  field  fields  growing  \\\n",
      "Doc 1           0   1    0    0           0     1      1       0        0   \n",
      "Doc 2           0   0    0    0           1     0      0       0        0   \n",
      "Doc 3           1   0    0    0           0     1      0       0        0   \n",
      "Doc 4           0   0    1    1           1     1      0       1        1   \n",
      "\n",
      "       intelligence  interdisciplinary  is  learning  machine  of  science  \\\n",
      "Doc 1             0                  1   1         0        0   0        1   \n",
      "Doc 2             1                  0   1         1        1   1        0   \n",
      "Doc 3             0                  0   0         1        1   0        1   \n",
      "Doc 4             1                  0   0         0        0   0        1   \n",
      "\n",
      "       subset  uses  \n",
      "Doc 1       0     0  \n",
      "Doc 2       1     0  \n",
      "Doc 3       0     1  \n",
      "Doc 4       0     0  \n",
      "\n",
      "\n",
      "1. The two words that appear most frequently across all documents are 'data' and 'science',both with a total count of three.\n",
      "\n",
      "2. There are eleven words that appear only once across all four documents: 'algorithms','an', 'and', 'are', 'field', 'fields', 'growing', 'interdisciplinary', 'of', 'subset', and 'uses'.\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Bag of Words\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents to create the BoW matrix\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "#Convert the result into a DataFrame for readability\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), \n",
    "                      columns=vectorizer.get_feature_names_out(), \n",
    "                      index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
    "\n",
    "print (\"\\nStep2: Bag of Words\")\n",
    "print (\"\\nBag of Words Model:\")\n",
    "print ()\n",
    "print (bow_df)\n",
    "print ()\n",
    "print (\"\\n1. The two words that appear most frequently across all documents are 'data' and 'science',\"\n",
    "       \" both with a total count of three.\")\n",
    "print (\"\\n2. There are eleven words that appear only once across all four documents: 'algorithms',\"\n",
    "      \" 'an', 'and', 'are', 'field', 'fields', 'growing', 'interdisciplinary', 'of', 'subset', and 'uses'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "951efa60-4270-482b-81ec-49b5e5dba85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: TF-IDF\n",
      "\n",
      "TF-IDF Representation\n",
      "\n",
      "       algorithms     an    and    are  artificial   data  field  fields  \\\n",
      "Doc 1       0.000  0.475  0.000  0.000       0.000  0.303  0.475   0.000   \n",
      "Doc 2       0.000  0.000  0.000  0.000       0.349  0.000  0.000   0.000   \n",
      "Doc 3       0.496  0.000  0.000  0.000       0.000  0.317  0.000   0.000   \n",
      "Doc 4       0.000  0.000  0.406  0.406       0.320  0.259  0.000   0.406   \n",
      "\n",
      "       growing  intelligence  interdisciplinary     is  learning  machine  \\\n",
      "Doc 1    0.000         0.000              0.475  0.374     0.000    0.000   \n",
      "Doc 2    0.000         0.349              0.000  0.349     0.349    0.349   \n",
      "Doc 3    0.000         0.000              0.000  0.000     0.391    0.391   \n",
      "Doc 4    0.406         0.320              0.000  0.000     0.000    0.000   \n",
      "\n",
      "          of  science  subset   uses  \n",
      "Doc 1  0.000    0.303   0.000  0.000  \n",
      "Doc 2  0.442    0.000   0.442  0.000  \n",
      "Doc 3  0.000    0.317   0.000  0.496  \n",
      "Doc 4  0.000    0.259   0.000  0.000  \n",
      "\n",
      "1. The terms that have the highest TF-IDF score in the first document (Doc 1) are 'an', 'field',and 'interdisciplinary', all with a score of 0.475.\n",
      "\n",
      "2. Some terms have a TF-IDF score of 0.000 in certain documents because the Term Frequency (TF) componentof the calculation is zero, meaning that specific word does not appear in that particular document.\n"
     ]
    }
   ],
   "source": [
    "#Step 3: TF-IDF\n",
    "\n",
    "# Create a TF-IDF representation of the documents using TfidfVectorizer.\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(documents)\n",
    "\n",
    "# Convert the resulting TF-IDF matrix into a DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=vectorizer_tfidf.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(\"\\nStep 3: TF-IDF\")\n",
    "print(\"\\nTF-IDF Representation\")\n",
    "print ()\n",
    "print(tfidf_df.round(3))\n",
    "print(\"\\n1. The terms that have the highest TF-IDF score in the first document (Doc 1) are 'an', 'field',\" \n",
    "        \" and 'interdisciplinary', all with a score of 0.475.\")\n",
    "print(\"\\n2. Some terms have a TF-IDF score of 0.000 in certain documents because the Term Frequency (TF) component\" \n",
    "      \" of the calculation is zero, meaning that specific word does not appear in that particular document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5a1183f-5020-4319-b6ff-616413c5a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step4: N-grams\n",
      "\n",
      "Bigram (N-gram) Representation\n",
      "\n",
      "       an interdisciplinary  and data  are growing  artificial intelligence  \\\n",
      "Doc 1                     1         0            0                        0   \n",
      "Doc 2                     0         0            0                        1   \n",
      "Doc 3                     0         0            0                        0   \n",
      "Doc 4                     0         1            1                        1   \n",
      "\n",
      "       data science  growing fields  intelligence and  \\\n",
      "Doc 1             1               0                 0   \n",
      "Doc 2             0               0                 0   \n",
      "Doc 3             1               0                 0   \n",
      "Doc 4             1               1                 1   \n",
      "\n",
      "       interdisciplinary field  is an  is subset  learning algorithms  \\\n",
      "Doc 1                        1      1          0                    0   \n",
      "Doc 2                        0      0          1                    0   \n",
      "Doc 3                        0      0          0                    1   \n",
      "Doc 4                        0      0          0                    0   \n",
      "\n",
      "       learning is  machine learning  of artificial  science are  science is  \\\n",
      "Doc 1            0                 0              0            0           1   \n",
      "Doc 2            1                 1              1            0           0   \n",
      "Doc 3            0                 1              0            0           0   \n",
      "Doc 4            0                 0              0            1           0   \n",
      "\n",
      "       science uses  subset of  uses machine  \n",
      "Doc 1             0          0             0  \n",
      "Doc 2             0          1             0  \n",
      "Doc 3             1          0             1  \n",
      "Doc 4             0          0             0  \n",
      "\n",
      "1. The bigram that is most frequent across all documents is 'data science',which appears three times (once in Doc 1, Doc 3, and Doc 4).\n",
      "\n",
      "2. Bigrams provide additional context compared to unigrams (single words) bycapturing the sequence and relationship between two adjacent words, which helpsdifferentiate meanings (e.g., 'data science' versus just 'data' or 'science').\n"
     ]
    }
   ],
   "source": [
    "#Step4: N-grams\n",
    "\n",
    "# Generate bigrams (2-word combinations) using CountVectorizer with ngram_range=(2, 2)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2))\n",
    "bigram_matrix = vectorizer_bigram.fit_transform(documents)\n",
    "\n",
    "# Convert the resulting bigram matrix into a DataFrame for readability\n",
    "bigram_df = pd.DataFrame(\n",
    "    bigram_matrix.toarray(),\n",
    "    columns=vectorizer_bigram.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print (\"\\nStep4: N-grams\")\n",
    "print(\"\\nBigram (N-gram) Representation\")\n",
    "print ()\n",
    "print(bigram_df)\n",
    "print (\"\\n1. The bigram that is most frequent across all documents is 'data science',\" \n",
    "       \" which appears three times (once in Doc 1, Doc 3, and Doc 4).\")\n",
    "print (\"\\n2. Bigrams provide additional context compared to unigrams (single words) by\" \n",
    "       \" capturing the sequence and relationship between two adjacent words, which helps\" \n",
    "       \" differentiate meanings (e.g., 'data science' versus just 'data' or 'science').\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "892a1eb2-4d8a-4a53-84a1-4f37d0ccae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5:Analyze Combined Representations\n",
      "\n",
      "Compare the results from BoW and TF-IDF:\n",
      "\n",
      "1. BoW simply counts how often a term appears (raw frequency), giving a high scoreto common words like 'is' (count of 2 in your data); in contrast, TF-IDF down-weights termsthat appear frequently across many documents, resulting in a low TF-IDF score for 'is'(0.374 in Doc 1) because it's not unique.\n",
      "\n",
      "2. TF-IDF is preferred when you need to focus on the most distinctive and significant termsin each document, rather than common words that offer little unique information. For instance,'interdisciplinary' only appears once, so TF-IDF gives it a high score (0.475 in Doc 1), highlightingits importance to that specific document.The high scores assigned to these unique terms help data analystsquickly identify the main topic or content of a document when performing tasks like information retrieval ordocument classification.\n",
      "\n",
      "Compare unigrams (single words) with bigrams:\n",
      "\n",
      "1. Bigrams capture relationships by treating an ordered pair of words as a single feature, showing wordsthat commonly appear adjacent to each other. This is important to people that analyze the dataset becauseit helps them understand context and differentiate between specific concepts or phrases that individual wordsmight miss.\n",
      "\n",
      "2. For example, the bigram 'data science' is a specific field, which adds more meaning than just analyzingthe individual words 'data' and 'science' separately.The bigram 'data science' adds more meaning than theindividual words 'data' and 'science' because it functions as a single, specific term that names an entireacademic discipline and professional field.\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Analyze Combined Representations\n",
    "\n",
    "print (\"\\nStep 5:Analyze Combined Representations\") \n",
    "print(\"\\nCompare the results from BoW and TF-IDF:\")\n",
    "print(\"\\n1. BoW simply counts how often a term appears (raw frequency), giving a high score\" \n",
    "      \" to common words like 'is' (count of 2 in your data); in contrast, TF-IDF down-weights terms\" \n",
    "      \" that appear frequently across many documents, resulting in a low TF-IDF score for 'is'\" \n",
    "      \" (0.374 in Doc 1) because it's not unique.\") \n",
    "print (\"\\n2. TF-IDF is preferred when you need to focus on the most distinctive and significant terms\" \n",
    "       \" in each document, rather than common words that offer little unique information. For instance,\" \n",
    "       \" 'interdisciplinary' only appears once, so TF-IDF gives it a high score (0.475 in Doc 1), highlighting\" \n",
    "       \" its importance to that specific document.The high scores assigned to these unique terms help data analysts\" \n",
    "       \" quickly identify the main topic or content of a document when performing tasks like information retrieval or\" \n",
    "       \" document classification.\")\n",
    "\n",
    "print(\"\\nCompare unigrams (single words) with bigrams:\")\n",
    "print (\"\\n1. Bigrams capture relationships by treating an ordered pair of words as a single feature, showing words\" \n",
    "       \" that commonly appear adjacent to each other. This is important to people that analyze the dataset because\" \n",
    "       \" it helps them understand context and differentiate between specific concepts or phrases that individual words\" \n",
    "       \" might miss.\")\n",
    "print (\"\\n2. For example, the bigram 'data science' is a specific field, which adds more meaning than just analyzing\" \n",
    "        \" the individual words 'data' and 'science' separately.The bigram 'data science' adds more meaning than the\" \n",
    "        \" individual words 'data' and 'science' because it functions as a single, specific term that names an entire\" \n",
    "        \" academic discipline and professional field.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
