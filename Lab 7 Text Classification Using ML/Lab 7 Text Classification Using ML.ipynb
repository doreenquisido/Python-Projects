{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13934da7-4af5-4b36-a821-d4a9581bc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "719be1a6-2e7d-4b9b-ade8-86a10a061235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  plot : two teen couples go to a church party ,...       neg\n",
      "1  the happy bastard's quick movie review \\ndamn ...       neg\n",
      "2  it is movies like these that make a jaded movi...       neg\n",
      "3   \" quest for camelot \" is warner bros . ' firs...       neg\n",
      "4  synopsis : a mentally unstable man undergoing ...       neg\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Data\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Load the reviews into a list of dictionaries\n",
    "reviews_data = []\n",
    "\n",
    "# 'neg' (negative) and 'pos' (positive) categories\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        # Extract the raw text and the category\n",
    "        reviews_data.append({\n",
    "            'review': movie_reviews.raw(fileid),\n",
    "            'sentiment': category # 'neg' or 'pos'\n",
    "        })\n",
    "\n",
    "# Define the DataFrame 'df'\n",
    "df = pd.DataFrame(reviews_data)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d075188a-e75a-4f38-ae25-d63a8e6c3199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 3 rows with original and processed text:\n",
      "                                              review  \\\n",
      "0  plot : two teen couples go to a church party ,...   \n",
      "1  the happy bastard's quick movie review \\ndamn ...   \n",
      "2  it is movies like these that make a jaded movi...   \n",
      "\n",
      "                                      processed_text sentiment  \n",
      "0  plot two teen couples go church party drink dr...       neg  \n",
      "1  happy bastard quick movie review damn y2k bug ...       neg  \n",
      "2  movies like make jaded movie viewer thankful i...       neg  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocess Data\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags \n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # 2. Remove punctuation and characters that are NOT letters (a-z) or numbers (0-9)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # 3. Tokenize the text (split into words)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 2. Remove stop words (Filter out common words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "df['processed_text'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nFirst 3 rows with original and processed text:\")\n",
    "print(df[['review', 'processed_text', 'sentiment']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36546cb3-6d24-4221-928d-bf5b2ee22f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature matrix (X_features): (2000, 5000)\n",
      "This represents 2000 documents and 5000 features (words).\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Convert Text Data into Numerical Features\n",
    "\n",
    "#1a. X will be the feature (processed text)\n",
    "X = df['processed_text']\n",
    "\n",
    "#1b. y will be the sentiment. Convert 'neg'/'pos' labels into 0/1 for the model\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'pos' else 0)\n",
    "\n",
    "#2. Initialize the TF-IDF Vectorizer and set the maximum number of features to 5000 as specified\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "#3. Fit and Transform the data\n",
    "X_features = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(f\"Shape of the feature matrix (X_features): {X_features.shape}\")\n",
    "print(f\"This represents {X_features.shape[0]} documents and {X_features.shape[1]} features (words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67beb751-91e8-4172-9f1b-fc5a2ae66075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size (70%): 1400 samples\n",
      "Testing set size (30%): 600 samples\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split Data into Training and Testing Sets\n",
    "\n",
    "# test_size=0.3 ensures a 70% train / 30% test split\n",
    "# random_state=42 ensures the split is the same every time you run the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.3, random_state=42,stratify=y)\n",
    "\n",
    "print(f\"Training set size (70%): {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size (30%): {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81eed41a-6aa9-4637-aa0d-12a1d4f39db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Training and Evaluation ---\n",
      "\n",
      "Training and Evaluating: Multinomial Naive Bayes (MNB)\n",
      "Accuracy: 0.8250\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negative (0)       0.81      0.85      0.83       300\n",
      "Positive (1)       0.84      0.80      0.82       300\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.83      0.82      0.82       600\n",
      "weighted avg       0.83      0.82      0.82       600\n",
      "\n",
      "--- Summary of Model Accuracies ---\n",
      "\n",
      "Training and Evaluating: Logistic Regression (LR)\n",
      "Accuracy: 0.8350\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negative (0)       0.85      0.81      0.83       300\n",
      "Positive (1)       0.82      0.86      0.84       300\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.84      0.83      0.83       600\n",
      "weighted avg       0.84      0.83      0.83       600\n",
      "\n",
      "--- Summary of Model Accuracies ---\n",
      "\n",
      "Training and Evaluating: Support Vector Machine (SVM)\n",
      "Accuracy: 0.8433\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negative (0)       0.85      0.84      0.84       300\n",
      "Positive (1)       0.84      0.85      0.84       300\n",
      "\n",
      "    accuracy                           0.84       600\n",
      "   macro avg       0.84      0.84      0.84       600\n",
      "weighted avg       0.84      0.84      0.84       600\n",
      "\n",
      "--- Summary of Model Accuracies ---\n",
      "\n",
      "Training and Evaluating: K-Nearest Neighbors (KNN)\n",
      "Accuracy: 0.6600\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negative (0)       0.68      0.61      0.64       300\n",
      "Positive (1)       0.65      0.71      0.68       300\n",
      "\n",
      "    accuracy                           0.66       600\n",
      "   macro avg       0.66      0.66      0.66       600\n",
      "weighted avg       0.66      0.66      0.66       600\n",
      "\n",
      "--- Summary of Model Accuracies ---\n",
      "                           Model Accuracy\n",
      "0  Multinomial Naive Bayes (MNB)   0.8250\n",
      "1       Logistic Regression (LR)   0.8350\n",
      "2   Support Vector Machine (SVM)   0.8433\n",
      "3      K-Nearest Neighbors (KNN)   0.6600\n"
     ]
    }
   ],
   "source": [
    "# A dictionary to store the results\n",
    "results = {}\n",
    "models = {\n",
    "    \"Multinomial Naive Bayes (MNB)\": MultinomialNB(),\n",
    "    \"Logistic Regression (LR)\": LogisticRegression(solver='liblinear', random_state=42),\n",
    "    \"Support Vector Machine (SVM)\": SVC(kernel='linear', random_state=42), \n",
    "    \"K-Nearest Neighbors (KNN)\": KNeighborsClassifier(n_neighbors=5) \n",
    "}\n",
    "\n",
    "print(\"--- Model Training and Evaluation ---\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining and Evaluating: {name}\")\n",
    "    \n",
    "    # 1. Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 2. Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 3. Compute and record accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # 4. Generate classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative (0)', 'Positive (1)'])\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Report': report\n",
    "    }\n",
    "    \n",
    "    # Print immediate results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    print(\"--- Summary of Model Accuracies ---\")\n",
    "accuracy_summary = {name: data['Accuracy'] for name, data in results.items()}\n",
    "summary_df = pd.DataFrame(accuracy_summary.items(), columns=['Model', 'Accuracy'])\n",
    "summary_df['Accuracy'] = summary_df['Accuracy'].map('{:.4f}'.format)\n",
    "print(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
